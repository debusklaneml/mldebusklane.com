library(lubridate)
library(plotly)
# Read in the data.
brewing_materials <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-31/brewing_materials.csv')
beer_taxed <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-31/beer_taxed.csv')
brewer_size <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-31/brewer_size.csv')
beer_states <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-31/beer_states.csv')
hops1 <- brewing_materials %>%
mutate(date = make_date(year, month)) %>%
group_by(date, type) %>%
summarize(avg = mean(month_current)) %>%
filter(!type %in% c('Total Grain products', 'Other', 'Total Non-Grain products', 'Total Used'))
time_series1 <- hops1 %>%
ggplot(aes(x = date, y = avg, group = type)) +
geom_line(aes(color = type)) +
theme(axis.text.x = element_text(angle = 90))
time_series1
hops2 <- brewing_materials %>%
mutate(date = make_date(year, month)) %>%
filter(date >= '2008-01-01' & date <= '2013-12-01') %>%
group_by(date, type) %>%
summarize(avg = mean(month_current)) %>% ungroup() %>%
filter(type == 'Malt and malt products') %>%
mutate(date = ymd(date, truncated = 1))
time_series2 <- hops2 %>%
ggplot(aes(x = date, y = avg, group = type)) +
geom_line(aes(color = type)) +
theme(axis.text.x = element_text(angle = 90))
time_series2
class(hops2)
library(tsibble)
tsibble <- as_tsibble(hops2, index = date, key = type)
class(tsibble$date)
head(tsibble$date)
head(tsibble)
tsibble <- tsibble %>%
mutate(date = yearmonth(date))
class(tsibble$date)
class(tsibble)
library(TSstudio)
seasonal <- ts(data = tsibble$avg, start = c(2008, 1), end = c(2013, 12), frequency = 12)
ts_plot(seasonal,
line.mode = "lines+markers",
Ygrid = T, slider = T,
title = "Malts Over Time",
Ytitle = "Millions of Barrels")
ts_cor(seasonal, seasonal_lags = 12, lag.max = 60)
ts_seasonal(seasonal, type = 'all')
library(forecastLM)
lm1 <- trainLM(input = tsibble,
y = "avg",
trend = list(linear = TRUE),
seasonal = "month")
summary(lm1$model)
lm2  <- trainLM(input = tsibble,
y = "avg",
trend = list(linear = TRUE),
seasonal = "month",
lags = c(1))
summary(lm2$model)
plot_res(lm2)
events <- list(outlier = c(as_date("2009-01-01"), as_date("2009-09-01"), as_date("2010-05-01"), as_date("2010-12-01"), as_date("2014-12-01"), as_date("2011-09-01")))
lm3  <- trainLM(input = seasonal,
y = "avg",
trend = list(log = TRUE),
seasonal = "month",
lags = c(1),
step = TRUE,
events = events)
summary(lm3$model)
plot_res(lm3)
# Using the lm3 prediction model.
fc3 <- forecastLM(lm3, h = 24)
# Plot it.
plot_fc(fc3, theme = 'classic')
ts_cor(tsibble, seasonal_lags = 12, lag.max = 60)
ts_cor(seasonal, seasonal_lags = 12, lag.max = 60)
ts_cor(seasonal, seasonal_lags = 12, lag.max = 48)
ts_cor(seasonal)
? ts_cor
library(TSstudio)
seasonal <- ts(data = tsibble$avg, start = c(2008, 1), end = c(2014, 1), frequency = 12)
ts_plot(seasonal,
line.mode = "lines+markers",
Ygrid = T, slider = T,
title = "Malts Over Time",
Ytitle = "Millions of Barrels")
ts_cor(seasonal)
library(TSstudio)
seasonal <- ts(data = tsibble$avg, start = c(2008, 1), end = c(2013, 12), frequency = 12)
ts_plot(seasonal,
line.mode = "lines+markers",
Ygrid = T, slider = T,
title = "Malts Over Time",
Ytitle = "Millions of Barrels")
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE,
echo = TRUE,  cache.lazy = FALSE,
fig.width = 8, fig.height = 7)
library(tidyverse)
library(janitor)
library(lubridate)
library(plotly)
# Read in the data.
brewing_materials <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-31/brewing_materials.csv')
beer_taxed <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-31/beer_taxed.csv')
brewer_size <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-31/brewer_size.csv')
beer_states <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-31/beer_states.csv')
hops1 <- brewing_materials %>%
mutate(date = make_date(year, month)) %>%
group_by(date, type) %>%
summarize(avg = mean(month_current)) %>%
filter(!type %in% c('Total Grain products', 'Other', 'Total Non-Grain products', 'Total Used'))
time_series1 <- hops1 %>%
ggplot(aes(x = date, y = avg, group = type)) +
geom_line(aes(color = type)) +
theme(axis.text.x = element_text(angle = 90))
time_series1
hops2 <- brewing_materials %>%
mutate(date = make_date(year, month)) %>%
filter(date >= '2008-01-01' & date <= '2013-12-01') %>%
group_by(date, type) %>%
summarize(avg = mean(month_current)) %>% ungroup() %>%
filter(type == 'Malt and malt products') %>%
mutate(date = ymd(date, truncated = 1))
time_series2 <- hops2 %>%
ggplot(aes(x = date, y = avg, group = type)) +
geom_line(aes(color = type)) +
theme(axis.text.x = element_text(angle = 90))
time_series2
class(hops2)
library(tsibble)
tsibble <- as_tsibble(hops2, index = date, key = type)
class(tsibble$date)
head(tsibble$date)
head(tsibble)
tsibble <- tsibble %>%
mutate(date = yearmonth(date))
class(tsibble$date)
class(tsibble)
library(TSstudio)
seasonal <- ts(data = tsibble$avg, start = c(2008, 1), end = c(2013, 12), frequency = 12)
ts_plot(seasonal,
line.mode = "lines+markers",
Ygrid = T, slider = T,
title = "Malts Over Time",
Ytitle = "Millions of Barrels")
ts_cor(seasonal)
blogdown:::serve_site()
servr::daemon_stop()
blogdown:::serve_site()
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE,
echo = TRUE,  cache.lazy = FALSE,
fig.width = 8, fig.height = 7)
library(tidyverse)
library(janitor)
library(lubridate)
library(plotly)
# Read in the data.
brewing_materials <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-31/brewing_materials.csv')
beer_taxed <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-31/beer_taxed.csv')
brewer_size <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-31/brewer_size.csv')
beer_states <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-31/beer_states.csv')
hops1 <- brewing_materials %>%
mutate(date = make_date(year, month)) %>%
group_by(date, type) %>%
summarize(avg = mean(month_current)) %>%
filter(!type %in% c('Total Grain products', 'Other', 'Total Non-Grain products', 'Total Used'))
time_series1 <- hops1 %>%
ggplot(aes(x = date, y = avg, group = type)) +
geom_line(aes(color = type)) +
theme(axis.text.x = element_text(angle = 90))
time_series1
hops2 <- brewing_materials %>%
mutate(date = make_date(year, month)) %>%
filter(date >= '2008-01-01' & date <= '2013-12-01') %>%
group_by(date, type) %>%
summarize(avg = mean(month_current)) %>% ungroup() %>%
filter(type == 'Malt and malt products') %>%
mutate(date = ymd(date, truncated = 1))
time_series2 <- hops2 %>%
ggplot(aes(x = date, y = avg, group = type)) +
geom_line(aes(color = type)) +
theme(axis.text.x = element_text(angle = 90))
time_series2
class(hops2)
library(tsibble)
tsibble <- as_tsibble(hops2, index = date, key = type)
class(tsibble$date)
head(tsibble$date)
head(tsibble)
tsibble <- tsibble %>%
mutate(date = yearmonth(date))
class(tsibble$date)
class(tsibble)
library(TSstudio)
seasonal <- ts(data = tsibble$avg, start = c(2008, 1), end = c(2013, 12), frequency = 12)
ts_plot(seasonal,
line.mode = "lines+markers",
Ygrid = T, slider = T,
title = "Malts Over Time",
Ytitle = "Millions of Barrels")
ts_cor(seasonal)
ts_seasonal(seasonal, type = 'all')
library(forecastLM)
lm1 <- trainLM(input = tsibble,
y = "avg",
trend = list(linear = TRUE),
seasonal = "month")
summary(lm1$model)
lm2  <- trainLM(input = tsibble,
y = "avg",
trend = list(linear = TRUE),
seasonal = "month",
lags = c(1))
summary(lm2$model)
plot_res(lm2)
events <- list(outlier = c(as_date("2009-01-01"), as_date("2009-09-01"), as_date("2010-05-01"), as_date("2010-12-01"), as_date("2014-12-01"), as_date("2011-09-01")))
lm3  <- trainLM(input = seasonal,
y = "avg",
trend = list(log = TRUE),
seasonal = "month",
lags = c(1),
step = TRUE,
events = events)
summary(lm3$model)
plot_res(lm3)
# Using the lm3 prediction model.
fc3 <- forecastLM(lm3, h = 24)
# Plot it.
plot_fc(fc3, theme = 'classic')
View(lm3)
? ts_cor
ts_cor(seasonal, seasonal = True)
ts_cor(seasonal, seasonal = TRUE)
ts_cor(seasonal)
ts_cor(seasonal, seasonal_lags = 4)
ts_cor(seasonal, lag.max = 36)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE,
echo = TRUE,  cache.lazy = FALSE)
library(tidyverse)
library(janitor)
library(lubridate)
library(plotly)
# Read in the data.
brewing_materials <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-31/brewing_materials.csv')
beer_taxed <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-31/beer_taxed.csv')
brewer_size <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-31/brewer_size.csv')
beer_states <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-31/beer_states.csv')
hops1 <- brewing_materials %>%
mutate(date = make_date(year, month)) %>%
group_by(date, type) %>%
summarize(avg = mean(month_current)) %>%
filter(!type %in% c('Total Grain products', 'Other', 'Total Non-Grain products', 'Total Used'))
time_series1 <- hops1 %>%
ggplot(aes(x = date, y = avg, group = type)) +
geom_line(aes(color = type)) +
theme(axis.text.x = element_text(angle = 90))
time_series1
hops2 <- brewing_materials %>%
mutate(date = make_date(year, month)) %>%
filter(date >= '2008-01-01' & date <= '2013-12-01') %>%
group_by(date, type) %>%
summarize(avg = mean(month_current)) %>% ungroup() %>%
filter(type == 'Malt and malt products') %>%
mutate(date = ymd(date, truncated = 1))
time_series2 <- hops2 %>%
ggplot(aes(x = date, y = avg, group = type)) +
geom_line(aes(color = type)) +
theme(axis.text.x = element_text(angle = 90))
time_series2
class(hops2)
library(tsibble)
tsibble <- as_tsibble(hops2, index = date, key = type)
class(tsibble$date)
head(tsibble$date)
head(tsibble)
tsibble <- tsibble %>%
mutate(date = yearmonth(date))
class(tsibble$date)
class(tsibble)
library(TSstudio)
seasonal <- ts(data = tsibble$avg, start = c(2008, 1), end = c(2013, 12), frequency = 12)
ts_plot(seasonal,
line.mode = "lines+markers",
Ygrid = T, slider = T,
title = "Malts Over Time",
Ytitle = "Millions of Barrels")
ts_cor(seasonal, lag.max = 36)
ts_seasonal(seasonal, type = 'all')
library(forecastLM)
lm1 <- trainLM(input = tsibble,
y = "avg",
trend = list(linear = TRUE),
seasonal = "month")
summary(lm1$model)
lm2  <- trainLM(input = tsibble,
y = "avg",
trend = list(linear = TRUE),
seasonal = "month",
lags = c(1))
summary(lm2$model)
plot_res(lm2)
events <- list(outlier = c(as_date("2009-01-01"), as_date("2009-09-01"), as_date("2010-05-01"), as_date("2010-12-01"), as_date("2014-12-01"), as_date("2011-09-01")))
lm3  <- trainLM(input = seasonal,
y = "avg",
trend = list(log = TRUE),
seasonal = "month",
lags = c(1),
step = TRUE,
events = events)
summary(lm3$model)
plot_res(lm3)
# Using the lm3 prediction model.
fc3 <- forecastLM(lm3, h = 24)
# Plot it.
plot_fc(fc3, theme = 'classic')
? plot_fc
# Plot it.
plot_fc(fc3, theme = 'classic',
title = "help")
# Plot it.
plot_fc(fc3, theme = 'classic')
blogdown:::serve_site()
blogdown:::serve_site()
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE,
echo = TRUE,  cache.lazy = FALSE)
library(tidyverse)
library(janitor)
library(tidylog)
# Read in the data, although I probably wont need it all.
critic <- readr::read_tsv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/critic.tsv')
user_reviews <- readr::read_tsv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/user_reviews.tsv')
items <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/items.csv')
villagers <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv')
library(tidyverse)
library(janitor)
library(tidylog)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE,
echo = TRUE,  cache.lazy = FALSE)
library(tidyverse)
library(janitor)
library(tidyLPA)
# Read in the data, although I probably wont need it all.
critic <- readr::read_tsv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/critic.tsv')
user_reviews <- readr::read_tsv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/user_reviews.tsv')
items <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/items.csv')
villagers <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv')
View(villagers)
cleaning <- villagers %>%
select(-row_n)
View(cleaning)
cleaning <- villagers %>%
select(-row_n) %>%
distinct()
cleaning <- villagers %>%
select(-row_n, -id, -phrase:-url)
View(villagers)
View(items)
View(villagers)
View(items)
View(user_reviews)
View(critic)
View(villagers)
View(villagers)
? separate
cleaning <- villagers %>%
select(-row_n, -id, -phrase:-url) %>% #remove unneeded columns to examine villager commonalities (profiles)
separate(birthday, into = c("month", "day"), sep = "-", remove = FALSE)
cleaning <- villagers %>%
select(-row_n, -id, -phrase:-url) %>% #remove unneeded columns to examine villager commonalities (profiles)
separate(birthday, into = c("month", "day"), sep = "-")
cleaning <- villagers %>%
select(-row_n, -id, -phrase:-url) %>% #remove unneeded columns to examine villager commonalities (profiles)
separate(birthday, into = c("month", "day"), sep = "-") %>%
select(-day) %>% # not likely needed herein
rename(id = name)
library(recipes)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE,
echo = TRUE,  cache.lazy = FALSE)
library(tidyverse)
library(janitor)
library(tidyLPA)
library(recipes)
# Read in the data, although I probably wont need it all.
critic <- readr::read_tsv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/critic.tsv')
user_reviews <- readr::read_tsv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/user_reviews.tsv')
items <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/items.csv')
villagers <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv')
cleaned <- villagers %>%
select(-row_n, -id, -phrase:-url) %>% #remove unneeded columns to examine villager commonalities (profiles)
separate(birthday, into = c("month", "day"), sep = "-") %>%
select(-day) %>% # not likely needed herein
rename(id = name)
? recipe
preped <- cleaned %>%
recipe(~ .) %>%
step_dummy(gender, species, personality, song) %>%
prep(training = cleaned) %>%
bake(newdata = cleaned)
preped <- cleaned %>%
recipe(~ .) %>%
step_dummy(gender, species, personality, song) %>%
prep(training = cleaned) %>%
bake(new_data = cleaned)
View(preped)
preped <- cleaned %>%
recipe(~ .) %>%
step_dummy(gender, species, personality, song, one_hot = TRUE) %>%
prep(training = cleaned) %>%
bake(new_data = cleaned)
preped <- cleaned %>%
recipe(~ .) %>%
step_string2factor(gender, species, personality, song, one_hot = TRUE) %>%
prep(training = cleaned) %>%
bake(new_data = cleaned)
preped <- cleaned %>%
recipe(~ .) %>%
step_string2factor(gender, species, personality, song) %>%
prep(training = cleaned) %>%
bake(new_data = cleaned)
View(preped)
View(preped)
preped <- cleaned %>%
recipe(~ .) %>%
step_string2factor(gender, species, personality, song) %>%
step_ordinalscore(gender, species, personality, song) %>%
prep(training = cleaned) %>%
bake(new_data = cleaned)
preped <- cleaned %>%
recipe(~ .) %>%
step_string2factor(gender, species, personality, song) %>%
step_integer(gender, species, personality, song) %>%
prep(training = cleaned) %>%
bake(new_data = cleaned)
preped <- cleaned %>%
recipe(~ .) %>%
#step_string2factor(gender, species, personality, song) %>%
step_integer(gender, species, personality, song) %>%
prep(training = cleaned) %>%
bake(new_data = cleaned)
preped <- cleaned %>%
recipe(~ .) %>%
#step_string2factor(gender, species, personality, song) %>%
step_integer(gender, species, personality, song) %>%
prep(training = cleaned) %>%
bake(new_data = cleaned)
View(items)
View(user_reviews)
View(critic)
# Read in the data, although I probably wont need it all.
tdf_winners <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-04-07/tdf_winners.csv')
View(tdf_winners)
install.packages('blogdown')
install.packages("blogdown")
install.packages(c("bayesplot", "brms", "broom", "dplyr", "foreign", "ggplot2", "Hmisc", "janitor", "nlme", "rlang", "testthat"))
blogdown::find_hugo('all')
blogdown::check_hugo()
blogdown::check_netlify()
blogdown::serve_site()
blogdown::config_Rprofile()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
install.packages('simdata')
set.seed(1234) #for reproducability
nG <- 20 #number of groups
nJ <- 30 #cluster size
W1 <- 2 #level 2 coeff
X1 <- 3 #level 1 coeff
tmp2 <- rnorm(nG) #generate 20 random numbers, m = 0, sd = 1
l2 <- rep(tmp2, each = nJ) #all units in l2 have the same value
group <- gl(nG, k = nJ) #creating cluster variable
tmp2 <- rnorm(nG) #error term for level 2
err2 <- rep(tmp2, each = nJ) #all units in l2 have the same value
l1 <- rnorm(nG * nJ) #total sample size is nG * nJ
err1 <- rnorm(nG * nJ) #level 1
#putting it all together
y <- W1 * l2 + X1 * l1 + err2 + err1
dat <- data.frame(y, group, l2, err2,l1, err1)
View(dat)
View(dat)
set.seed(1234) #for reproducability
nG <- 250 #number of groups
nJ <- 18 #cluster size
W1 <- 2 #level 2 coeff
X1 <- 3 #level 1 coeff
tmp2 <- rnorm(nG) #generate 20 random numbers, m = 0, sd = 1
l2 <- rep(tmp2, each = nJ) #all units in l2 have the same value
group <- gl(nG, k = nJ) #creating cluster variable
tmp2 <- rnorm(nG) #error term for level 2
err2 <- rep(tmp2, each = nJ) #all units in l2 have the same value
l1 <- rnorm(nG * nJ) #total sample size is nG * nJ
err1 <- rnorm(nG * nJ) #level 1
#putting it all together
y <- W1 * l2 + X1 * l1 + err2 + err1
dat <- data.frame(y, group, l2, err2,l1, err1)
l2
blogdown:::new_post_addin()
blogdown::stop_server()
blogdown::serve_site()
blogdown:::new_post_addin()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
install.packages('AMSMAth')
blogdown::serve_site()
blogdown::serve_site()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
# Load packages
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, tidymodels, multilevelmod, mnormt, ranger, randomForest, tidylog)
youtube <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-03-02/youtube.csv') %>%
filter(!is.na(view_count))
skimr::skim(youtube)
# Initial training test splits
initial_split_youtube <- initial_split(youtube, prop = 0.6)
initial_split_youtube
# split it up accordingly
yt_training <- training(initial_split_youtube)
yt_testing <- testing(initial_split_youtube)
# set up cross-validation
yt_vfold <- vfold_cv(yt_training, v = 10, repeats = 2)
# This is using an initial formula. I'll update this later if needed.
yt_recipe <-
recipe(view_count ~ .,
data = youtube) %>%
# Add some pre-processing steps
step_rm(superbowl_ads_dot_com_url, youtube_url, id, kind, etag,
dislike_count, favorite_count, comment_count, published_at, title,
description, thumbnail, category_id, channel_title) %>%
# step_filter(!is.na(view_count)) %>%
step_dummy(brand) %>%
step_mutate_at(funny:use_sex, fn = ~as.numeric(.)) %>%
step_normalize(all_numeric()) %>%
step_knnimpute(all_predictors())
# step_mutate(funny = as.numeric(funny))
yt_recipe
# See what this training data might look like IRL
recipe_data <- yt_recipe %>%
prep(yt_training) %>%
juice()
recipe_data
rf_model <-
rand_forest() %>%
set_args(mtry = tune(), trees(range = c(1L, 2000L))) %>%
set_engine("ranger", importance = "impurity") %>%
set_mode("regression")
# set the workflow
rf_workflow <- workflow() %>%
# add the recipe
add_recipe(yt_recipe) %>%
# add the model
add_model(rf_model)
rf_workflow
# specify which values eant to try
rf_grid <- expand.grid(mtry = c(5, 6, 7))
# extract results
rf_tune_results <- rf_workflow %>%
tune_grid(resamples = yt_vfold, #CV object
grid = rf_grid, # grid of values to try
metrics = metric_set(rmse) # metrics we care about
)
# print results
rf_tune_results %>%
collect_metrics()
param_final <- rf_tune_results %>%
select_best(metric = "rmse")
param_final
rf_workflow <- rf_workflow %>%
finalize_workflow(param_final)
rf_fit <- rf_workflow %>%
# fit on the training set and evaluate on test set
last_fit(initial_split())
rf_fit <- rf_workflow %>%
# fit on the training set and evaluate on test set
last_fit(initial_split_youtube)
rf_fit
test_performance <- rf_fit %>% collect_metrics()
test_performance
param_final <- rf_tune_results %>%
select_best(metric = "rmse")
param_final
rf_workflow <- rf_workflow %>%
finalize_workflow(param_final)
rf_workflow
rf_fit <- rf_workflow %>%
# fit on the training set and evaluate on test set
last_fit(initial_split_youtube)
test_performance <- rf_fit %>% collect_metrics()
test_performance
rf_fit$id
rf_fit$.notes
View(rf_fit)
View(rf_fit[[3]][[1]])
View(rf_fit[[4]][[1]])
View(rf_fit[[5]][[1]])
View(rf_fit[[6]][[1]])
final_model <- fit(rf_workflow, youtube)
final_model
ranger_obj <- pull_workflow_fit(final_model)$fit
ranger_obj
ranger_obj$variable.importance
vi_df <- as.data.frame(ranger_obj$variable.importance)
View(vi_df)
options(scipen=999)
vi_df <- as.data.frame(ranger_obj$variable.importance)
vi_df
vi_df <- as.data.frame(ranger_obj$variable.importance) %>%
arrange(ranger_obj$variable.importance)
vi_df
vi_df <- as.data.frame(ranger_obj$variable.importance) %>%
arrange(-ranger_obj$variable.importance)
vi_df
# Initial training test splits
initial_split_youtube <- initial_split(youtube, prop = 0.6)
initial_split_youtube
# split it up accordingly
yt_training <- training(initial_split_youtube)
yt_testing <- testing(initial_split_youtube)
# set up cross-validation
yt_vfold <- vfold_cv(yt_training, v = 10, repeats = 2)
# This is using an initial formula. I'll update this later if needed.
yt_recipe <-
recipe(view_count ~ .,
data = youtube) %>%
# Add some pre-processing steps
step_rm(superbowl_ads_dot_com_url, youtube_url, id, kind, etag,
dislike_count, favorite_count, comment_count, published_at, title,
description, thumbnail, category_id, channel_title, like_count) %>%
# step_filter(!is.na(view_count)) %>%
step_dummy(brand) %>%
step_mutate_at(funny:use_sex, fn = ~as.numeric(.)) %>%
step_normalize(all_numeric()) %>%
step_knnimpute(all_predictors())
# step_mutate(funny = as.numeric(funny))
yt_recipe
# See what this training data might look like IRL
recipe_data <- yt_recipe %>%
prep(yt_training) %>%
juice()
recipe_data
rf_model <-
rand_forest() %>%
set_args(mtry = tune(), trees(range = c(1L, 2000L))) %>%
set_engine("ranger", importance = "impurity") %>%
set_mode("regression")
# set the workflow
rf_workflow <- workflow() %>%
# add the recipe
add_recipe(yt_recipe) %>%
# add the model
add_model(rf_model)
rf_workflow
# specify which values eant to try
rf_grid <- expand.grid(mtry = c(5, 6, 7))
# extract results
rf_tune_results <- rf_workflow %>%
tune_grid(resamples = yt_vfold, #CV object
grid = rf_grid, # grid of values to try
metrics = metric_set(rmse) # metrics we care about
)
# print results
rf_tune_results %>%
collect_metrics()
param_final <- rf_tune_results %>%
select_best(metric = "rmse")
param_final
rf_workflow <- rf_workflow %>%
finalize_workflow(param_final)
rf_workflow
rf_fit <- rf_workflow %>%
# fit on the training set and evaluate on test set
last_fit(initial_split_youtube)
test_performance <- rf_fit %>% collect_metrics()
test_performance
final_model <- fit(rf_workflow, youtube)
final_model
ranger_obj <- pull_workflow_fit(final_model)$fit
ranger_obj
vi_df <- as.data.frame(ranger_obj$variable.importance) %>%
arrange(-ranger_obj$variable.importance)
vi_df
corr(youtube$view_count, youtube$year)
cor(youtube$view_count, youtube$year)
youtube %>%
select(view_count, year) %>%
ggplot(aes(x = year, y = view_count)) +
geom_line()
youtube %>%
select(view_count, year) %>%
ggplot(aes(x = year, y = view_count)) +
geom_bar()
youtube %>%
select(view_count, year) %>%
ggplot(aes(x = year, y = view_count)) +
geom_col()
games <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-03-16/games.csv')
View(games)
install.packages('dataedu')
remotes::install_github("data-edu/dataedu")
library(dataedu)
a <- data(package = "dataedu")
a$result[ , 3:4]
View(a)
data('Walkthrough 01 - Student Motivation (Processed)')
dataedu::Walkthrough 01 - Student Motivation (Processed)
dataedu::'Walkthrough 01 - Student Motivation (Processed)'
here <- dataedu::'Walkthrough 01 - Student Motivation (Processed)'
? sim_multi_mem
? brms::sim_multi_mem
my_sim_data <- function(
n_subj     = 100,   # number of subjects
n_ingroup  =  25,   # number of ingroup stimuli
n_outgroup =  25,   # number of outgroup stimuli
beta_0     = 800,   # grand mean
beta_1     =  50,   # effect of category
omega_0    =  80,   # by-item random intercept sd
tau_0      = 100,   # by-subject random intercept sd
tau_1      =  40,   # by-subject random slope sd
rho        = 0.2,   # correlation between intercept and slope
sigma      = 200) { # residual (standard deviation)
items <- data.frame(
item_id = seq_len(n_ingroup + n_outgroup),
category = rep(c("ingroup", "outgroup"), c(n_ingroup, n_outgroup)),
X_i = rep(c(-0.5, 0.5), c(n_ingroup, n_outgroup)),
O_0i = rnorm(n = n_ingroup + n_outgroup, mean = 0, sd = omega_0))
# variance-covariance matrix
cov_mx  <- matrix(
c(tau_0^2,             rho * tau_0 * tau_1,
rho * tau_0 * tau_1, tau_1^2            ),
nrow = 2, byrow = TRUE)
subjects <- data.frame(subj_id = seq_len(n_subj),
MASS::mvrnorm(n = n_subj,
mu = c(T_0s = 0, T_1s = 0),
Sigma = cov_mx))
crossing(subjects, items)  %>%
mutate(e_si = rnorm(nrow(.), mean = 0, sd = sigma),
RT = beta_0 + T_0s + O_0i + (beta_1 + T_1s) * X_i + e_si) %>%
select(subj_id, item_id, category, X_i, RT)
}
test <- my_sim_data(n_subj = 1000)
View(test)
styler:::style_selection()
pacman::p_load(tidyverse, tidymodels, multilevelmod, mnormt, ranger, randomForest, simglm, tidylog)
? simulate
? simulate_fixed
? reg_weight
browseVignettes()
set.seed(90210)
sim_arguments <- list(
formula = y ~ 1 + weight + age + sex + (1 | neighborhood),
reg_weights = c(4, -0.03, 0.2, 0.33),
fixed = list(weight = list(var_type = 'continuous', mean = 180, sd = 30),
age = list(var_type = 'ordinal', levels = 30:60),
sex = list(var_type = 'factor', levels = c('male', 'female'))),
randomeffect = list(int_neighborhood = list(variance = 8, var_level = 2)),
sample_size = list(level1 = 10, level2 = 20)
)
nested_data <- sim_arguments %>%
simulate_fixed(data = NULL, .) %>%
simulate_randomeffect(sim_arguments) %>%
simulate_error(sim_arguments) %>%
generate_response(sim_arguments)
head(nested_data, n = 10)
set.seed(90210)
sim_arguments <- list(
formula = y ~ 1 + weight + age + sex + (1 | neighborhood),
reg_weights = c(4, -0.03, 0.2, 0.33),
fixed = list(weight = list(var_type = 'continuous', mean = 180, sd = 30),
age = list(var_type = 'continuous', mean = 37, sd = 18),
sex = list(var_type = 'factor', levels = c('male', 'female'))),
randomeffect = list(int_neighborhood = list(variance = 8, var_level = 2)),
sample_size = list(level1 = 75, level2 = 20)
)
nested_data <- sim_arguments %>%
simulate_fixed(data = NULL, .) %>%
simulate_randomeffect(sim_arguments) %>%
simulate_error(sim_arguments) %>%
generate_response(sim_arguments)
head(nested_data, n = 10)
set.seed(90210)
sim_arguments <- list(
formula = y ~ 1 + weight + age + sex + (1 | neighborhood),
reg_weights = c(4, -0.03, 0.2, 0.33),
fixed = list(weight = list(var_type = 'continuous', mean = 180, sd = 30),
age = list(var_type = 'continuous', mean = 37, sd = 18),
sex = list(var_type = 'factor', levels = c('male', 'female'))),
randomeffect = list(int_neighborhood = list(variance = 8, var_level = 2)),
sample_size = list(level1 = 75, level2 = 20)
)
nested_data <- sim_arguments %>%
simulate_fixed(data = NULL, .) %>%
simulate_randomeffect(sim_arguments) %>%
simulate_error(sim_arguments) %>%
generate_response(sim_arguments)
nexted_data
set.seed(90210)
sim_arguments <- list(
formula = y ~ 1 + weight + age + sex + (1 | neighborhood),
reg_weights = c(4, -0.03, 0.2, 0.33),
fixed = list(weight = list(var_type = 'continuous', mean = 180, sd = 30),
age = list(var_type = 'continuous', mean = 37, sd = 18),
sex = list(var_type = 'factor', levels = c('male', 'female'))),
randomeffect = list(int_neighborhood = list(variance = 8, var_level = 2)),
sample_size = list(level1 = 75, level2 = 20)
)
nested_data <- sim_arguments %>%
simulate_fixed(data = NULL, .) %>%
simulate_randomeffect(sim_arguments) %>%
simulate_error(sim_arguments) %>%
generate_response(sim_arguments)
nested_data
set.seed(90210)
sim_arguments <- list(
formula = y ~ 1 + weight + age + sex + (1 | neighborhood),
reg_weights = c(4, -0.03, 0.2, 0.33),
fixed = list(weight = list(var_type = 'continuous', mean = 180, sd = 30),
age = list(var_type = 'continuous', mean = 37, sd = 18),
sex = list(var_type = 'factor', levels = c('male', 'female'))),
randomeffect = list(int_neighborhood = list(variance = 8, var_level = 2)),
sample_size = list(level1 = 75, level2 = 25)
)
nested_data <- sim_arguments %>%
simulate_fixed(data = NULL, .) %>%
simulate_randomeffect(sim_arguments) %>%
simulate_error(sim_arguments) %>%
generate_response(sim_arguments)
nested_data
set.seed(90210)
sim_arguments <- list(
formula = y ~ 1 + weight + age + sex + (1 | neighborhood),
reg_weights = c(4, -0.03, 0.2, 0.33),
fixed = list(weight = list(var_type = 'continuous', mean = 180, sd = 30),
age = list(var_type = 'continuous', mean = 37, sd = 18),
sex = list(var_type = 'factor', levels = c('male', 'female'))),
randomeffect = list(int_neighborhood = list(variance = 8, var_level = 2)),
sample_size = list(level1 = 75, level2 = 25)
)
nested_data <- sim_arguments %>%
simulate_fixed(data = NULL, .) %>%
simulate_randomeffect(sim_arguments) %>%
simulate_error(sim_arguments) %>%
generate_response(sim_arguments)
nested_data
skimr::skim(youtube)
skimr::skim(nested_data)
nested_data_clean <- nested_data %>%
select(y, weight, age, sex, neighborhood)
skimr::skim(nested_data_clean)
view(nested_data_clean)
# Initial training test splits
initial_split_nested <- initial_split(nested_data_clean, prop = 0.6)
initial_split_nested
# split it up accordingly
nest_training <- training(initial_split_nested)
nest_testing <- testing(initial_split_nested)
# set up cross-validation
nest_vfold <- vfold_cv(nest_training, v = 10, repeats = 2)
# This is using an initial formula. I'll update this later if needed.
nest_recipe <-
recipe(y ~ .,
data = nested_data_clean) %>%
# Add some pre-processing steps
step_normalize(all_numeric())
nest_recipe
# See what this training data might look like IRL
recipe_data <- nest_recipe %>%
prep(nest_training) %>%
juice()
recipe_data
# This is using an initial formula. I'll update this later if needed.
nest_recipe <-
recipe(y ~ .,
data = nested_data_clean) %>%
# Add some pre-processing steps
step_normalize(all_numeric(-neighborhood))
nest_recipe
# See what this training data might look like IRL
recipe_data <- nest_recipe %>%
prep(nest_training) %>%
juice()
# This is using an initial formula. I'll update this later if needed.
nest_recipe <-
recipe(y ~ .,
data = nested_data_clean) %>%
# Add some pre-processing steps
step_normalize(all_numeric(), -neighborhood)
nest_recipe
# See what this training data might look like IRL
recipe_data <- nest_recipe %>%
prep(nest_training) %>%
juice()
recipe_data
skimr::skim(recipe_data)
