---
title: "TidyTuesday - Beer Production - 2020 - Wk 10"
author: admin
date: '2020-04-07'
slug:  
categories: [Code]
tags:
  - R
  - TidyTuesday
subtitle: ''  
summary: 'A look at beer production'
authors: [M. L. DeBusk-Lane]
lastmod: '2020-04-07T07:57:09-05:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
output: 
  html_document:
    theme: journal
    highlight: zenburn

---
```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, 
                      echo = TRUE, dpi = 300, cache.lazy = FALSE,
                      fig.width = 8, fig.height = 7)
```

This week's effort will dive into beer production across America. You may find more informaiton at [Alcohol and Tobacco Tax and Trade Bureau (TTB)](https://www.ttb.gov/beer/statistics). 

```{r}
library(tidyverse)
library(janitor)
library(readxl)
library(tidylog)
library(ggdark)
library(ggforce)
library(rvest)
library(glue)
library(patchwork)
library(stringi)
library(lubridate)

# Read in the data. 
brewing_materials <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-31/brewing_materials.csv')
beer_taxed <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-31/beer_taxed.csv')
brewer_size <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-31/brewer_size.csv')
beer_states <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-31/beer_states.csv')

```

Let's begin with a bit of data prep. I'm most interested in how this looks longitudinally, so I used `make_date` to bring together the year and month into a date variable--primarily for proper sorting. 

```{r}
hops1 <- brewing_materials %>%
  mutate(date = make_date(year, month)) %>%
  group_by(date, type) %>%
  summarize(avg = mean(month_current)) %>%
  filter(!type %in% c('Total Grain products', 'Other', 'Total Non-Grain products', 'Total Used')) 
```

```{r}
time_series1 <- hops1 %>%
  ggplot(aes(x = date, y = avg, group = type)) + 
  geom_line(aes(color = type)) + 
  theme(axis.text.x = element_text(angle = 90))
time_series1
```

Clearly something does not look right after 2016. From what I can find, it appears they changed the reporting criteria or structure. Although I was looking for some type of diverging trend to indicate a change in types of beer popularity (my hypothesis), I'm not sure that is evident. The `Hops (dry)` variable obvious does something crazy in 2015-2016, but I'm not sure I trust that much of an increase beween 2014 and 2015. 

Nevertheless, lets keep going with an effort towards modeling this time series between 2008 and the end of 2014 (somewhat cleanish/valid data). Also, just to zoom in a bit, I'd like to examine the seasonality of `Malt and malt products`. 

```{r}
hops2 <- brewing_materials %>%
  mutate(date = make_date(year, month)) %>%
  #unite("testdate", year:month, sep = "-") %>%
  #mutate(date = zoo::as.yearmon(testdate, "%Y-%m")) %>%
  filter(date >= '2008-01-01' & date <= '2014-01-01') %>%
   #  %>%
 # mutate(date1 = as_date(as.yearmon(date, "%Y-%b")))
  group_by(date, type) %>%  
  summarize(avg = mean(month_current)) %>% ungroup() %>%
  filter(type == 'Malt and malt products') %>%
  mutate(date = ymd(date, truncated = 1))
  #mutate(date2 = as.Date(date))
```

```{r}
time_series2 <- hops2 %>%
  ggplot(aes(x = date, y = avg, group = type)) + 
  geom_line(aes(color = type)) + 
  theme(axis.text.x = element_text(angle = 90))
time_series2
```

```{r}
class(hops2)
```

To model this, I'd like to push this time series into a `tsibble` to classify it as temporal data. 

```{r}
library(tsibble)
tsibble <- as_tsibble(hops2, index = date, key = type)
```


```{r}
class(tsibble$date)
```

I fought with this for a good while. Lesson learned. Although this variable `ts$date` identifies as a 'Date' variable, it is not in the right or correct interval. 

Lets take a look. Notice the structure of the `date` variable below, it is in `%Y-%m-%d'. 

In the end, this forecasts daily, not monthly. We must coerce this variable's interval as a month. 

```{r}
head(tsibble)
```

In this instance, I knew I wanted to use the `forecastLM` developmental package which  takes a tsibble or ts object. However, it does not have internal checks to determine your data align with the interval for which you've situated. 

In my case, I did not originally situate an interval for the tsibble object I created and ran into problems during forecasting. 

How to set a `year month` interval in tsibble using the `yearmonth` function:

```{r}
tsibble <- tsibble %>%
  mutate(date = yearmonth(date))
```
```{r}
class(tsibble$date)
```

This allows forecastLM to make predictions on a monthly basis, not daily... 

```{r}
class(tsibble)
```

Visualize using the TSstudio package from `Rami Krispin`, which takes a ts object as input for graphics. So lets make a ts object, as opposed to the tsibble. 
```{r , out.width='100%', out.height='100%'}
library(TSstudio) 
seasonal <- ts(data = tsibble$avg, start = c(2008, 1), end = c(2014, 1), frequency = 12)
ts_plot(seasonal,
        title = "Malts Over Time",
        Ytitle = "Millions of Barrels")
```

```{r , out.width='100%', out.height='100%'}
ts_seasonal(seasonal, type = "all")
```



From this, it appears there is a strong montly sesonality--with a bit of variability. However, there does appear to be a noticable decline across the entire year as time moves. This trend is also evident in the plot above this one, whereby there appears to be a fairly linear decline over time. 

I'll use a developmental R package `forecastLM` to examine this trend. You can find more details [here](https://github.com/RamiKrispin/forecastLM). 

```{r}
library(forecastLM)
lm1 <- trainLM(input = tsibble,
               y = "avg",
               trend = list(linear = TRUE),
               seasonal = "month")
```

```{r}
summary(lm1$model)
```

```{r}
lm1.1 <- trainLM(input = seasonal,
               y = "avg",
               trend = list(linear = TRUE),
               seasonal = "month")
```

```{r}
summary(lm1.1$model)
```


```{r , out.width='100%', out.height='100%'}
plot_res(lm1.1)
```

It appears there is a bit of residual correlations, so lets see if we can improve the model by accounting for the lag. 

```{r}
lm2  <- trainLM(input = tsibble,
               y = "avg",
               trend = list(linear = TRUE),
               seasonal = "month", 
               lags = c(1))
```

```{r}

```


```{r}
summary(lm2$model)
```

```{r , out.width='100%', out.height='100%'}
plot_res(lm2)
```

```{r}
events <- list(outlier = c(as_date("2009-01-01"), as_date("2009-09-01"), as_date("2010-05-01"), as_date("2010-12-01"), as_date("2014-12-01"), as_date("2011-09-01")))
lm3  <- trainLM(input = seasonal,
               y = "avg",
               trend = list(log = TRUE),
               seasonal = "month", 
               lags = c(1),
               step = TRUE,
               events = events)
               #scale = "log")
```

```{r}
summary(lm3$model)
```

```{r , out.width='100%', out.height='100%'}
plot_res(lm3)
```

Now that we have a prediction model that accounts for roughly 87% of the variability, we can look and see what this may have looked like with somewhat clean data or consistent reporting. 

```{r} 
# Using the lm3 prediction model. 
fc3 <- forecastLM(lm3, h = 24)
```

```{r , out.width='100%', out.height='100%'}
# Plot it.
plot_fc(fc3, theme = 'darkBlue')
```

```{r}
library(forecast)

```

